name: "wazuhl_DQN"

layer {
  name: "state_input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: ${MINIBATCH_SIZE}
      dim: ${NUMBER_OF_FEATURES}
    }
  }
}

layer {
  name: "context_input"
  type: "Input"
  top: "context"
  input_param {
    shape {
      dim: ${CONTEXT_SIZE}
      dim: ${MINIBATCH_SIZE}
    }
  }
}
layer {
  name: "clip_input"
  type: "Input"
  top: "clip"
  input_param {
    shape {
      dim: ${CONTEXT_SIZE}
      dim: ${MINIBATCH_SIZE}
    }
  }
}

layer {
  name: "action_input"
  type: "Input"
  top: "data_action"
  input_param {
    shape {
      dim: ${MINIBATCH_SIZE}
      dim: ${NUMBER_OF_ACTIONS}
    }
  }
  include {
    phase: TRAIN
  }
}

layer {
  name: "flatten"
  type: "Flatten"
  top: "flatten_data_action"
  bottom: "data_action"
  include {
    phase: TRAIN
  }
}

layer {
  name: "flatten_context"
  type: "Flatten"
  bottom: "context"
  top: "flat_context"
}

layer {
  name: "flatten_clip"
  type: "Flatten"
  bottom: "clip"
  top: "flat_clip"
}

layer {
  name: "embedding"
  type: "Embed"
  bottom: "flat_context"
  top: "context_embedding"
  param {
   lr_mult: 1
  }
  embed_param {
   bias_term: false
   input_dim: ${CONTEXT_ALPHABET_SIZE} # alphabet size
   num_output: ${CONTEXT_EMBEDDING_SIZE}
   weight_filler {
    type: "uniform"
    min: -0.1
    max: 0.1
   }
 }
}

layer {
  name: "lstm"
  type: "LSTM"
  bottom: "context_embedding"
  bottom: "flat_clip"
  top: "lstm"
  recurrent_param {
    num_output: ${CONTEXT_LSTM_SIZE}
  }
}

# lstm only works in seq2seq mode and we need only the last element
layer {
  name: "crop_sequence"
  type: "Slice"
  bottom: "lstm"
  top: "redundant_seq"
  top: "last_lstm"
  slice_param {
    axis: 0
    slice_point: ${CONTEXT_LSTM_LAST_IDX}
  }
}

layer {
  name: "reshaped_lstm"
  type: "Flatten"
  bottom: "last_lstm"
  top: "context_encoded"
  flatten_param {
    axis: 0
    end_axis: 1
  }
}

layer {
  name: "norm"
  type: "BatchNorm"
  bottom: "data"
  top: "norm_data"
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm_data"
  top: "ip1"
  inner_product_param {
    num_output: ${HIDDEN_SIZE_1}
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "relu1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "relu1"
  top: "ip2"
  inner_product_param {
    num_output: ${HIDDEN_SIZE_2}
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "relu2"
}

layer {
  name: "concat"
  type: "Concat"
  bottom: "relu2"
  bottom: "context_encoded"
  top: "state_encoded"
  concat_param {
    axis: 1
  }
}


layer {
  name: "action_ip1"
  type: "InnerProduct"
  bottom: "state_encoded"
  top: "action_ip1"
  inner_product_param {
    num_output: ${ACTION_HIDDEN_SIZE}
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "action_relu1"
  type: "ReLU"
  bottom: "action_ip1"
  top: "action_relu1"
}
layer {
  name: "action_ip2"
  type: "InnerProduct"
  bottom: "action_relu1"
  top: "action_ip2"
  inner_product_param {
    num_output: ${NUMBER_OF_ACTIONS}
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "action_tanh"
  type: "TanH"
  bottom: "action_ip2"
  top: "action_tanh"
}
layer {
  name: "scale"
  type: "Power"
  bottom: "action_tanh"
  top: "action_scaled"
  power_param {
    power: 1
    scale: 20
    shift: 0
  }
}
layer {
  name: "action_bias"
  type: "Bias"
  bottom: "action_scaled"
  top: "A_advantage"
  bias_param {
    filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "mean"
  type: "Reduction"
  bottom: "A_advantage"
  top: "mean_advantage"
  reduction_param {
    operation: MEAN
    axis: 1
  }
}
layer {
  name: "reshape"
  type: "Reshape"
  bottom: "mean_advantage"
  top: "rmean_advantage"
  reshape_param {
    shape {
      dim: 0  # copy the dimension from below
      dim: 1
    }
  }
}
layer {
  name: "negation"
  type: "Power"
  bottom: "rmean_advantage"
  top: "negative_mean_advantage"
  power_param {
    power: 1
    scale: -1
    shift: 0
  }
}
layer {
  name: "tile_mean"
  type: "Tile"
  bottom: "negative_mean_advantage"
  top: "nm_advantage"
  tile_param {
    tiles: ${NUMBER_OF_ACTIONS}
    axis: 1
  }
}
layer {
  name: "delta"
  type: "Eltwise"
  bottom: "A_advantage"
  bottom: "nm_advantage"
  top: "delta_advantage"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "state_encoded"
  top: "ip3"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "tanh"
  type: "TanH"
  bottom: "ip3"
  top: "tanh"
}
layer {
  name: "scale"
  type: "Power"
  bottom: "tanh"
  top: "scaled"
  power_param {
    power: 1
    scale: 20
    shift: 0
  }
}
layer {
  name: "bias"
  type: "Bias"
  bottom: "scaled"
  top: "V_value"
  bias_param {
    filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "tile"
  type: "Tile"
  bottom: "V_value"
  top: "V_values"
  tile_param {
    tiles: ${NUMBER_OF_ACTIONS}
  }
}
layer {
  name: "sum"
  type: "Eltwise"
  bottom: "delta_advantage"
  bottom: "V_values"
  top: "Q_values"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "argmax"
  type: "ArgMax"
  bottom: "Q_values"
  top: "argmax"
  argmax_param {
    out_max_val: true
  }
  include: {
    phase: TEST
}
}

layer {
  name: "eltwise"
  type: "Eltwise"
  bottom: "Q_values"
  bottom: "flatten_data_action"
  top: "filtered_Q_values"
  eltwise_param {
    operation: PROD
  }
  include {
    phase: TRAIN
  }
}

layer {
  name: "value_input"
  type: "Input"
  top: "value_input"
  include {
    phase: TRAIN
  }
  input_param {
    shape {
      dim: ${MINIBATCH_SIZE}
      dim: ${NUMBER_OF_ACTIONS}
    }
  }
}
layer {
  name: "silence"
  type: "Silence"
  bottom: "redundant_seq"
}

layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "filtered_Q_values"
  bottom: "value_input"
  top: "loss"
  include {
    phase: TRAIN
  }
}
